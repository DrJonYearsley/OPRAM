# =============================================================================
# Julia program to produce the final results for the OPRAM app. 
# This program does the following:
#  extract results from model jld2 files 
#  import the multi-year average of degree day model results
#  calculate corresponding anomaly for each year
#  aggregate results across hectads
#  export results to CSV files
#
# The degree day model results are generatde by OPRAM_main_program.jl
# The multi-year average is generated by OPRAM_calculate_average.jl
#
#
# Author: Jon Yearsley  (jon.yearsley@ucd.ie)
# Date: April 2025
#
# =============================================================================
# =============================================================================

# Load the required packages
using CSV
using DataFrames
using Statistics
using Dates
using JLD2
using Distributed
using SharedArrays
using TOML


include("OPRAM_io_functions.jl");
include("OPRAM_processing_functions.jl");


# ============================================================================================
# =============== Import parameter values =======================================================



# Model parameters are stored in a TOML file https://toml.io/en/
if length(ARGS) == 1
    nNodes, run_params, species_setup, paths = import_parameters(ARGS[1])

elseif length(ARGS) == 0 & isfile("parameters.toml")
    nNodes, run_params, species_setup, paths = import_parameters("parameters.toml")

else
    @error "No parameter file given"
end



# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Set species parameters from the parameter file (can be more than one species)
if species_setup.speciesStr[1] == "all"
    @info "Importing all species from the species file" * string(species_setup.speciesStr)
    species_params = import_species(species_setup.speciesFile, species_setup.speciesStr[1])
else
    @info "Importing species from the species file: " * string(species_setup.speciesStr)
    species_params = [import_species(species_setup.speciesFile, species_setup.speciesStr[s]) for s in eachindex(species_setup.speciesStr)]
end


# =============== End of parameter setup =====================================================
# ============================================================================================




# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Import location data and thin it using thinFactor
# (needed to add the hectad codes to the 10km output) 
grid = read_grid(run_params)

# Create easting and northings of bottom left of a hectad
grid.east_hectad = convert.(Int32, floor.(grid.east ./ 1e4) .* 1e4)
grid.north_hectad = convert.(Int32, floor.(grid.north ./ 1e4) .* 1e4)


# =========================================================
# =========================================================
# Import data for each species and for each time period
# Calculate the anomaly and then save the result at the 1km and 10km scales

for s in eachindex(species_params)

    # Find directory matching the species name in run_params
    regex = Regex(replace(lowercase(species_params[s].species_name),
        r"\s" => "\\w"))  # Replace spaces with reg expression
    speciesName = filter(x -> occursin(regex, x), readdir(paths.resultsDir))
    if length(speciesName) > 1
        @error "More than one species name found"
    elseif length(speciesName) == 0
        @error "Species not found"
    end

    @info "Importing data for $(speciesName[1])"


    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # Make directory for the output if one doesn't exist
    if !isdir(paths.outDir)
        @info "        Making directory " * paths.outDir
        mkpath(paths.outDir)
    end

    # Specify the output directory using the species name
    if !isdir(joinpath(paths.outDir, speciesName[1]))
        @info "        Making directory " * speciesName[1]
        mkpath(joinpath(paths.outDir, speciesName[1]))
    end


    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # Import the 1km results for the species and years

    # Initialise vector of files to import
    inFiles = Vector{String}(undef, 0)
    yearVec = Vector{Int64}(undef, 0)
    yearStrVec = Vector{String}(undef, 0)
    if run_params.TRANSLATE_future
        # Create vector of files to import for future climates (TRANSLATE project)

        for j in eachindex(run_params.futurePeriod)
            # Find future year from the futurePeriod
            if occursin(r"(2021|2035|2050)", run_params.futurePeriod[j])
                Year = 2035
            elseif occursin(r"(2051|2065|2070)", run_params.futurePeriod[j])
                Year = 2065
            else
                @error "Future period not recognised"
            end

            # Set starting dates as the first of every month   
            dates = [Date(Year, m, 01) for m in 1:12]

            for i in eachindex(run_params.rcp)
                filename = filter(x -> occursin(r"^" * speciesName[1] * "_" * run_params.country *
                                                "_rcp" * run_params.rcp[i] * "_" * run_params.futurePeriod[j] * "_1km_" * run_params.method * ".jld2", x),
                    readdir(joinpath(paths.resultsDir, speciesName[1])))

                if length(filename) > 1
                    @error "More than one input file found"
                end

                push!(inFiles, joinpath(paths.resultsDir, speciesName[1], filename[1]))
                push!(yearStrVec, "rcp" * run_params.rcp[i] * "_" * run_params.futurePeriod[j])
                push!(yearVec, Year)
            end
        end
    else

        # Create vector of files to import for past climates
        for y in eachindex(run_params.years)
            filename = filter(x -> occursin(r"^" * speciesName[1] * "_" * run_params.country * "_" * string(run_params.years[y]) * "_1km_" * run_params.method * ".jld2", x),
                readdir(joinpath(paths.resultsDir, speciesName[1])))

            if length(filename) > 1
                @error "More than one input file found"
            else
                push!(inFiles, joinpath(paths.resultsDir, speciesName[1], filename[1]))
            end
            push!(yearVec, run_params.years[y])
            push!(yearStrVec, string(run_params.years[y]))
        end

    end

    # =========================================================
    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # Import the averaged data
    aggFile = joinpath(paths.resultsDir, speciesName[1], "average_" * speciesName[1] * "_" *
                                                         run_params.thirty_years * "_1km_" * run_params.method * ".csv")
    d_agg = CSV.read(aggFile, DataFrame, missingstring="NA")


    # If spatial locations are missing from d_gg then fill them in with missing values (or 0 for nGenerations)
    if length(unique(d_agg.ID)) < nrow(grid)
        @info "Some spatial location data missing in d_agg"
    end
    # =========================================================



    # =========================================================
    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    # Import and process individual year data

    for f in eachindex(inFiles)

        # Import the data from the jld2 files
        println("  ")
        @info "Importing data for " * yearStrVec[f]
        df_tmp = read_OPRAM_JLD2(inFiles[f], yearVec[f], grid)


        if run_params.TRANSLATE_future
            # If TRANSLATE future data then calculate the average across replicates

            # Group data frame by location and then starting Date 
            df_group = groupby(df_tmp, [:ID, :startMonth])

            # Calculate average over all replicates
            df_1km = combine(df_group,
                :nGenerations => (x -> if all(isa.(x, Missing))
                   missing
                else
                    mean(skipmissing(x))
                end) => :nGenerations,          # Mean generations
                :emergeDOY => (x -> if all(isa.(x, Missing))
                    missing
                else
                    mean(skipmissing(x))
                end) => :emergeDOY)                # Mean emergence DOY

            df_1km.startDate = Date.(yearVec[f], df_1km.startMonth, 1)

            df_group = nothing  # Remove df_group

        else
            # If not TRANSLATE future data then just use the imported data
            df_1km = df_tmp
        end
        df_tmp = nothing  # Remove df_tmp



        # Check that all spatial locations are present in df_1km
        startMonth = unique(df_1km.startMonth)
        for m in eachindex(startMonth)
            idx = findall(df_1km.startMonth .== startMonth[m])
            if length(idx) < nrow(grid)
                @info "Some spatial location data missing in df_1km"
            end
        end





        # =========================================================
        # =========================================================
        # Combine the multi year median with the original data and calculate anomalies

        @info "---- Calculating anomalies"

        # Combine origninal data frame with 30 year average
        leftjoin!(df_1km, select(d_agg, Not([:east, :north])), on=[:ID, :startMonth])

        # Calculate anomalies
        transform!(df_1km, [:nGenerations, :nGenerations_median] => ((a, b) -> a .- b) => :nGenerations_anomaly)
        transform!(df_1km, [:emergeDOY, :emergeDOY_median] => ((a, b) -> a .- b) => :emergeDOY_anomaly)


        # Add year and countyID into df_1km
        df_1km.year = year.(df_1km.startDate)
        leftjoin!(df_1km, grid[:, [:ID, :countyID, :east, :north]], on=:ID)


        # =========================================================
        # =========================================================
        # Create 10km summary ---------

        @info "---- Aggregating data to 10km resolution"
        df_10km = aggregate_to_hectad(df_1km)
 
        # Add in hectad ID
        leftjoin!(df_10km, unique(grid[:, [:hectad, :east_hectad, :north_hectad]]), on=[:east_hectad, :north_hectad])

        # =========================================================
        # =========================================================
        # Replace missing anomalies with zeros if required ---------
        if run_params.missing2zero

            @info "---- Replacing missing anomaly values with zeros"

            # For nGenerations_anomaly and emergeDOY_anomaly (1km)
            idx_ngen = ismissing.(df_1km.nGenerations_anomaly)
            if any(idx_ngen)
                df_1km.nGenerations_anomaly[idx_ngen] .= 0.0
            end

            idx_ngen = ismissing.(df_1km.emergeDOY_anomaly)
            if any(idx_ngen)
                df_1km.emergeDOY_anomaly[idx_ngen] .= 0.0
            end

            # For nGenerations_anomaly_median and emergeDOY_anomaly_median (10km)
            idx_ngen = ismissing.(df_10km.nGenerations_anomaly_median)
            if any(idx_ngen)
                df_10km.nGenerations_anomaly_median[idx_ngen] .= 0.0
            end

            idx_ngen = ismissing.(df_10km.emergeDOY_anomaly_median)
            if any(idx_ngen)
                df_10km.emergeDOY_anomaly_median[idx_ngen] .= 0.0
            end
        end


        # =========================================================
        # =========================================================
        # Export the anomaly and multi year average using a separate file for every year and every county

        @info "---- Writing 1km results to CSV files"
        save_OPRAM_1km_CSV(df_1km, speciesName[1], yearStrVec[f], paths)




        # =========================================================
        # =========================================================
        # Export 10km summary for the anomaly and multi year average
        @info "---- Writing 10km data for year " * yearStrVec[f]
        save_OPRAM_10km_CSV(df_10km, speciesName[1], yearStrVec[f], paths)
    end


    # Clear data for this species
    df_10km = nothing
    df_group = nothing
    df_1km = nothing
    df_agg = nothing


    # Print blank line
    println(" ")
end
